#
# Note to self: I'm not actually using the Serverless integration on their site,
# it turns out that streaming logs is super expensive, relatively speaking.
# I was looking at about $6/mo, which is more than the rest of my AWS bill combined.
#
org: dmuth
app: peco
service: peco-api
frameworkVersion: '3'

provider:
  name: aws

  # This is a faster method, and will be the default in Serverless 4
  deploymentMethod: direct

  runtime: python3.10

  # Don't keep old versions of functions.
  versionFunctions: false

  # Cloudwatch log retention for 30 days
  logRetentionInDays: 30

  environment:
    STAGE: ${sls:stage}
    DYNAMO_TABLE_NAME: !Ref pecoOutagesTable
    DYNAMO_TABLE_NAME_ARCHIVE: !Ref pecoOutagesTableArchive


  iam:
    role:
      statements:
        - Effect: "Allow"
          Action:
            - dynamodb:Query
            - dynamodb:Scan
            - dynamodb:GetItem
            - dynamodb:PutItem
            - dynamodb:UpdateItem
            - dynamodb:DeleteItem
          Resource: [
            "arn:aws:dynamodb:${aws:region}:${aws:accountId}:table/${self:custom.dynamoTableName}",
            "arn:aws:dynamodb:${aws:region}:${aws:accountId}:table/${self:custom.dynamoTableName}/index/Hour"
            ]
        #
        # Limit our Lambdas to just being able to write to the Archive table.
        #
        - Effect: "Allow"
          Action:
            - dynamodb:PutItem
          Resource: [
            "arn:aws:dynamodb:${aws:region}:${aws:accountId}:table/${self:custom.dynamoTableNameArchive}"
            ]

  stackTags:
    project: "peco"
    stage: "${sls:stage}"

  httpApi:
    cors: true


functions:

  test:
    handler: api/testing.main
    #url: true
    #events:
    #  - httpApi:
    #      path: /test
    #      method: get

  #
  # This function fetches the latest PECO statuses from DynamoDB.
  #
  peco:
    handler: api/peco.get_status
    events:
      - httpApi:
          path: /peco
          method: get

  #
  # Get recent status.  At this time, the default is 12 (1 hour).
  #
  peco_recent:
    handler: api/peco.get_status_recent
    events:
      - httpApi: 
          path: /peco/recent
          method: get

  #
  # Live status from PECO (for testing/development purposes)
  #
  peco_live:
    handler: api/peco.get_status_live
    #events: # Debugging
    #  - httpApi: 
    #      path: /peco/live
    #      method: get


  #
  # This function fetches PECO's status periodically and writes it to 
  # a DynamoDB table.
  #
  cron:
    handler: api/cron.main
    events:
      - schedule: 
          rate: rate(1 minute)
          enabled: true
      #- httpApi: # Debugging
      #    path: /cron
      #    method: get


#
# Our resources are in CloudFormation syntax
#
resources:

  Resources:
    #
    # Our "main" DynamoDB table.  It stores a subset of data for what the website uses.
    # With less data stored here, reads will take up less read units.
    #
    # This is, BTW, raw CloudFormation config right here.
    #
    pecoOutagesTable:
      Type: AWS::DynamoDB::Table
      Properties:
        #
        # If I mess around with the indexes, I will have to briefly rename this table
        # to something else, deploy, then change the name back and deploy a second time.
        #
        # If I don't do that, Serverless (or CloudFormation) doesn't properly pick up 
        # what I'm trying to do and will throw an error when it tries to delete 
        # a non-existant table.
        #
        # What a stupid bug.
        #
        TableName: ${self:custom.dynamoTableName}
        #TableName: ${self:custom.dynamoTableName}-tmp-01 # Uncomment this when changing the table schema

        # Set to on-demand capacity mode
        BillingMode: PAY_PER_REQUEST  

        # Prevent accidental deletion
        #DeletionProtectionEnabled: true

        #
        # Set up our attribute definitions for the Partition Key and Sort Key.
        #
        AttributeDefinitions:
          - AttributeName: Date
            AttributeType: S
          - AttributeName: DateTime
            AttributeType: S
          - AttributeName: Hour
            AttributeType: S

        KeySchema:
          - AttributeName: Date
            KeyType: HASH
          - AttributeName: DateTime
            KeyType: RANGE

        GlobalSecondaryIndexes:
          - IndexName: Hour
            KeySchema:
            - AttributeName: Date
              KeyType: HASH
            - AttributeName: Hour
              KeyType: RANGE
            Projection:
              ProjectionType: "ALL"

        PointInTimeRecoverySpecification:
          PointInTimeRecoveryEnabled: true

        TimeToLiveSpecification:
          AttributeName: "ttl"
          Enabled: true


    #
    # This table holds the raw data that we got back from PECO.
    # This table won't be used by the website, but rather for archival purposes.
    #
    pecoOutagesTableArchive:
      Type: AWS::DynamoDB::Table
      Properties:
        TableName: ${self:custom.dynamoTableNameArchive}
        # Set to on-demand capacity mode
        BillingMode: PAY_PER_REQUEST  
        # Prevent accidental deletion
        #DeletionProtectionEnabled: true
        #
        # We're just having a single key for the archive
        #
        AttributeDefinitions:
          - AttributeName: DateTime
            AttributeType: S
        KeySchema:
          - AttributeName: DateTime
            KeyType: HASH


    #
    # Create our bucket.  Public access is blocked by default.
    #
    AssetsBucket:
      Type: AWS::S3::Bucket
      Properties:
        BucketName: dmuth-peco-${sls:stage}


plugins:
  - serverless-python-requirements
  #
  # https://www.serverless.com/plugins/serverless-plugin-resource-tagging
  #
  - serverless-plugin-resource-tagging
  #
  # Used to sync our static assets to S3. 
  #
  # https://www.serverless.com/plugins/serverless-s3-sync
  #
  - serverless-s3-sync


custom:

  stage: "${sls:stage}"
  #
  # This is the source of truth for Dynamo table names.
  #
  dynamoTableName: "peco-outages-${sls:stage}"
  dynamoTableNameArchive: "peco-outages-archive-${sls:stage}"


  s3Sync:
    #
    # Don't deploy when doing "sls deploy", require "sls s3sync" instead.
    #
    noSync: true

    buckets:
    - bucketName: dmuth-peco-${sls:stage}
      localDir: hugo/public
      deleteRemoved: true




